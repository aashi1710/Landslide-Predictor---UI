{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashi1710/Landslide-Predictor-UI/blob/main/Landslide_mappping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLXz7H2mczM1",
        "outputId": "6af50492-add4-41ee-9f74-7509783e3034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.21)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.13.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO7s7xH0dNTm",
        "outputId": "6bb4118a-c5e6-4e10-aa4b-ea9f7a2d47f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvFepSDsxP1A",
        "outputId": "9d4cc905-20e1-49cc-99ed-1a5bdbdb6936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.8/dist-packages (3.19.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from gradio) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.25.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.8/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.8/dist-packages (from gradio) (23.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.8/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.4)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.1.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (10.4)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.8/dist-packages (from gradio) (0.3.3)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.8/dist-packages (from gradio) (0.0.5)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.8/dist-packages (from gradio) (3.17)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from gradio) (2.0.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.8/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2023.1.0)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.8/dist-packages (from gradio) (0.23.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.8/dist-packages (from gradio) (0.92.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py~=1.0 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (1.0.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.26.0,>=0.25.0 in /usr/local/lib/python3.8/dist-packages (from fastapi->gradio) (0.25.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (1.5.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (0.16.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from python-multipart->gradio) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (4.0.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.8/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (5.10.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.8/dist-packages (from linkify-it-py~=1.0->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=4.2.0->gradio) (3.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2Gkxgi1dVFG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import h5py\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KB92qgOMxPNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaF_F14c2PJ5",
        "outputId": "c15beaf2-0204-41da-b843-0e248bd2951b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFITZ2Nl580x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(r'/content/gdrive/MyDrive/TrainData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osX6aBsx8qpu"
      },
      "source": [
        "Testing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRp3K64-5Pfu"
      },
      "outputs": [],
      "source": [
        "# Testing the dataset\n",
        "path_single = '/content/gdrive/MyDrive/TrainData/TrainData/img/image_100.h5'\n",
        "path_single_mask ='/content/gdrive/MyDrive/TrainData/TrainData/mask/mask_100.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "rJUaSZtq5Ul-",
        "outputId": "21dcab79-6007-4c4d-8a50-eec769a2336a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cebd200303e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_single\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/content/gdrive/MyDrive/TrainData/TrainData/img/image_100.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "f_data = np.zeros((1, 128,128, 3))\n",
        "with h5py.File(path_single) as hdf:\n",
        "    ls = list(hdf.keys())\n",
        "    print(\"ls\", ls)\n",
        "    data = np.array(hdf.get('img'))\n",
        "    print(\"input data shape:\", data.shape)\n",
        "    plt.imshow(data[:, :, 3:0:-1])\n",
        "\n",
        "    data_red = data[:, :, 3]\n",
        "    data_green = data[:, :, 2]\n",
        "    data_blue = data[:, :, 1]\n",
        "    data_nir = data[:, :, 7]\n",
        "    data_rgb = data[:, :, 3:0:-1]\n",
        "    data_ndvi = np.divide(data_nir - data_red,np.add(data_nir, data_red))\n",
        "    f_data[0, :, :, 0] =data_ndvi\n",
        "    f_data[0, :, :, 1] = data[:, :, 12]\n",
        "    f_data[0, :, :, 2] = data[:, :, 13]\n",
        "\n",
        "    print(\"data ndvi shape \", data_ndvi.shape, \"f_data shape: \", f_data.shape)\n",
        "    plt.imshow(data_ndvi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu8s1YvG8dhj"
      },
      "outputs": [],
      "source": [
        "with h5py.File(path_single_mask) as hdf:\n",
        "    ls = list(hdf.keys())\n",
        "    print(\"ls\", ls)\n",
        "    data = np.array(hdf.get('mask'))\n",
        "    print(\"input data shape:\", data.shape)\n",
        "    plt.imshow(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZgyWtnX9WSe"
      },
      "source": [
        "Using All DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtaIJllH9hJX"
      },
      "outputs": [],
      "source": [
        "path_single = r'/content/gdrive/MyDrive/TrainData/TrainData/img/image_104.h5'\n",
        "path_single_mask = r'/content/gdrive/MyDrive/TrainData/TrainData/mask/mask_104.h5'\n",
        "TRAIN_PATH = r\"/content/gdrive/MyDrive/TrainData/TrainData/img/*\"\n",
        "TRAIN_MASK = r'/content/gdrive/MyDrive/TrainData/TrainData/mask/*'\n",
        "\n",
        "TRAIN_XX = np.zeros((300, 128, 128, 6))\n",
        "TRAIN_YY = np.zeros((300, 128, 128, 1))\n",
        "all_train = sorted(glob.glob(TRAIN_PATH))\n",
        "all_mask = sorted(glob.glob(TRAIN_MASK))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sUgtJBK-QUu"
      },
      "source": [
        "**Train with RGB, NDVI, DEM, and Slope**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J16lFNrM-YGv"
      },
      "outputs": [],
      "source": [
        "#testing for google colab GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPxteNQI2T3E"
      },
      "outputs": [],
      "source": [
        "for i, (img, mask) in enumerate(zip(all_train, all_mask)):\n",
        "    print(i, img, mask)\n",
        "    with h5py.File(img) as hdf:\n",
        "        ls = list(hdf.keys())\n",
        "        data = np.array(hdf.get('img'))\n",
        "\n",
        "        # assign 0 for the nan value\n",
        "        data[np.isnan(data)] = 0.000001\n",
        "\n",
        "        # to normalize the data\n",
        "        mid_rgb = data[:, :, 1:4].max() / 2.0\n",
        "        mid_slope = data[:, :, 12].max() / 2.0\n",
        "        mid_elevation = data[:, :, 13].max() / 2.0\n",
        "\n",
        "        # ndvi calculation\n",
        "        data_red = data[:, :, 3]\n",
        "        data_nir = data[:, :, 7]\n",
        "        data_ndvi = np.divide(data_nir - data_red,np.add(data_nir, data_red))\n",
        "\n",
        "        # final array\n",
        "        TRAIN_XX[i, :, :, 0] = 1 - data[:, :, 3] / mid_rgb  #RED\n",
        "        TRAIN_XX[i, :, :, 1] = 1 - data[:, :, 2] / mid_rgb #GREEN\n",
        "        TRAIN_XX[i, :, :, 2] = 1 - data[:, :, 1] / mid_rgb #BLUE\n",
        "        TRAIN_XX[i, :, :, 3] = data_ndvi #NDVI\n",
        "        TRAIN_XX[i, :, :, 4] = 1 - data[:, :, 12] / mid_slope #SLOPE\n",
        "        TRAIN_XX[i, :, :, 5] = 1 - data[:, :, 13] / mid_elevation #ELEVATION\n",
        "\n",
        "\n",
        "    with h5py.File(mask) as hdf:\n",
        "        ls = list(hdf.keys())\n",
        "        data=np.array(hdf.get('mask'))\n",
        "        TRAIN_YY[i, :, :, 0] = data\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7I-hogdUmrs"
      },
      "outputs": [],
      "source": [
        "# TRAIN_XX_n = TRAIN_XX / TRAIN_XX.max()\n",
        "TRAIN_XX[np.isnan(TRAIN_XX)] = 0.000001\n",
        "print(TRAIN_XX.min(), TRAIN_XX.max(), TRAIN_YY.min(), TRAIN_YY.max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvuHcfzgUwjb"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "  y_true = tf.cast(y_true, tf.float32)\n",
        "  y_pred = tf.math.sigmoid(y_pred)\n",
        "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
        "  denominator = tf.reduce_sum(y_true + y_pred)\n",
        "\n",
        "  return 1 - numerator / denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGdZmM2HU099"
      },
      "outputs": [],
      "source": [
        "img=10\n",
        "fig,(ax1,ax2, ax3, ax4, ax5)= plt.subplots(1,5,figsize=(15,10))\n",
        "\n",
        "\n",
        "ax1.set_title(\"RGB image\")\n",
        "ax2.set_title(\"NDVI\")\n",
        "ax3.set_title(\"Slope\")\n",
        "ax4.set_title(\"Elevation\")\n",
        "ax5.set_title(\"Mask\")\n",
        "ax1.imshow(TRAIN_XX[img, :, :, 0:3])\n",
        "ax2.imshow(TRAIN_XX[img, :, :, 3])\n",
        "ax3.imshow(TRAIN_XX[img, :, :, 4])\n",
        "ax4.imshow(TRAIN_XX[img, :, :, 5])\n",
        "ax5.imshow(TRAIN_YY[img, :, :, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2snUvbbhVAmN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(TRAIN_XX, TRAIN_YY, test_size=0.2, shuffle= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEg5TN4vViEi"
      },
      "outputs": [],
      "source": [
        "img=10\n",
        "fig,(ax1,ax2, ax3, ax4, ax5)= plt.subplots(1,5,figsize=(15,10))\n",
        "\n",
        "ax1.set_title(\"RGB image\")\n",
        "ax2.set_title(\"NDVI\")\n",
        "ax3.set_title(\"Slope\")\n",
        "ax4.set_title(\"Elevation\")\n",
        "ax5.set_title(\"Mask\")\n",
        "ax1.imshow(x_train[img, :, :, 0:3])\n",
        "ax2.imshow(x_train[img, :, :, 3])\n",
        "ax3.imshow(x_train[img, :, :, 4])\n",
        "ax4.imshow(x_train[img, :, :, 5])\n",
        "ax5.imshow(y_train[img, :, :, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y94E70lxVqBm"
      },
      "outputs": [],
      "source": [
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDv_yOZ2VuEd"
      },
      "outputs": [],
      "source": [
        "# to release some memory, delete the unnecessary variable\n",
        "del TRAIN_XX\n",
        "del TRAIN_YY\n",
        "del all_train\n",
        "del all_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQKYDE1rVzV8"
      },
      "outputs": [],
      "source": [
        "img=110\n",
        "fig,(ax1,ax2, ax3, ax4)= plt.subplots(1,4,figsize=(15,10))\n",
        "\n",
        "\n",
        "ax1.set_title(\"RGB image\")\n",
        "ax2.set_title(\"NDVI\")\n",
        "ax3.set_title(\"SLOPE\")\n",
        "ax4.set_title(\"Mask\")\n",
        "ax1.imshow(x_train[img, :, :, 0:3])\n",
        "ax2.imshow(x_train[img, :, :, 3])\n",
        "ax3.imshow(x_train[img, :, :, 4])\n",
        "ax4.imshow(y_train[img, :, :, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL6QGXeDXb2s"
      },
      "outputs": [],
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "# precision\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "#f1 score\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufwtXbUiWghd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def unet_model(IMG_WIDTH, IMG_HIGHT, IMG_CHANNELS):\n",
        "    inputs = tf.keras.layers.Input((IMG_WIDTH, IMG_HIGHT, IMG_CHANNELS))\n",
        "\n",
        "    # Converted inputs to floating\n",
        "    #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "\n",
        "\n",
        "    #Contraction path\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "    #Expansive path\n",
        "    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z3N4Jn9WsWq"
      },
      "outputs": [],
      "source": [
        "model = unet_model(128, 128, 6)\n",
        "# model.summary()\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", monitor=\"val_f1_m\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "# earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_m', patience=10, verbose=1, mode='max')\n",
        "\n",
        "callbacks = [\n",
        "    # earlyStopping,\n",
        "    checkpointer\n",
        "    ]\n",
        "history = model.fit(x_train, y_train, batch_size=16,\n",
        "          epochs=100,\n",
        "          verbose = 2,\n",
        "          validation_data=(x_valid, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model.save(\"model_save.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxFcHBwvW2Iw"
      },
      "outputs": [],
      "source": [
        "loss, accuracy, f1_score, precision, recall = model.evaluate(x_valid, y_valid, verbose=0)\n",
        "print(loss, accuracy, f1_score, precision, recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XywVq054m6rz"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSjgS9vAmvOR"
      },
      "outputs": [],
      "source": [
        "fig,((ax11,ax12),(ax13,ax14)) = plt.subplots(2,2,figsize=(20,15))\n",
        "ax11.plot(history.history['loss'])\n",
        "ax11.plot(history.history['val_loss'])\n",
        "ax11.title.set_text('Unet model loss')\n",
        "ax11.set_ylabel('loss')\n",
        "ax11.set_xlabel('epoch')\n",
        "ax11.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax12.plot(history.history['precision_m'])\n",
        "ax12.plot(history.history['val_precision_m'])\n",
        "ax12.set_title('Unet model precision')\n",
        "ax12.set_ylabel('precision')\n",
        "ax12.set_xlabel('epoch')\n",
        "ax12.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax13.plot(history.history['recall_m'])\n",
        "ax13.plot(history.history['val_recall_m'])\n",
        "ax13.set_title('Unet model recall')\n",
        "ax13.set_ylabel('recall')\n",
        "ax13.set_xlabel('epoch')\n",
        "ax13.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax14.plot(history.history['f1_m'])\n",
        "ax14.plot(history.history['val_f1_m'])\n",
        "ax14.set_title('Unet model f1')\n",
        "ax14.set_ylabel('f1')\n",
        "ax14.set_xlabel('epoch')\n",
        "ax14.legend(['train', 'validation'], loc='upper left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OKvztlFnB4M"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "pred_img = model.predict(x_valid)\n",
        "pred_img = (pred_img > threshold).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYvjEO6XnE4m"
      },
      "outputs": [],
      "source": [
        "img = 16\n",
        "fig,(ax1,ax2,ax3)= plt.subplots(1,3,figsize=(15,10))\n",
        "ax1.imshow(pred_img[img, :, :, 0])\n",
        "ax1.set_title(\"Predictions\")\n",
        "ax2.imshow(y_valid[img, :, :, 0])\n",
        "ax2.set_title(\"Label\")\n",
        "ax3.imshow(x_valid[img, :, :, 0:3])\n",
        "ax3.set_title('Training Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJnWhKjU0hYb"
      },
      "source": [
        "validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eudsfJh0fwU"
      },
      "outputs": [],
      "source": [
        "validation_url = r'/content/gdrive/MyDrive/ValidData/img/*.h5'\n",
        "img_val = sorted(glob.glob(validation_url))\n",
        "\n",
        "VAL_XX = np.zeros((245, 128, 128, 6))\n",
        "mask_name = []\n",
        "for i, img in enumerate(img_val):\n",
        "    print(i, img)\n",
        "    mask_name.append(img.split('/')[-1].replace('image', 'mask'))\n",
        "    with h5py.File(img) as hdf:\n",
        "        ls = list(hdf.keys())\n",
        "        data = np.array(hdf.get('img'))\n",
        "\n",
        "        # assign 0 for the nan value\n",
        "        data[np.isnan(data)] = 0.000001\n",
        "\n",
        "        # to normalize the data\n",
        "        mid_rgb = data[:, :, 1:4].max() / 2.0\n",
        "        mid_slope = data[:, :, 12].max() / 2.0\n",
        "        mid_elevation = data[:, :, 13].max() / 2.0\n",
        "\n",
        "        # ndvi calculation\n",
        "        data_red = data[:, :, 3]\n",
        "        data_nir = data[:, :, 7]\n",
        "        data_ndvi = np.divide(data_nir - data_red,np.add(data_nir, data_red))\n",
        "\n",
        "        # final array\n",
        "        VAL_XX[i, :, :, 0] = 1 - data[:, :, 3] / mid_rgb #RED\n",
        "        VAL_XX[i, :, :, 1] = 1 - data[:, :, 2] / mid_rgb #GREEN\n",
        "        VAL_XX[i, :, :, 2] = 1 - data[:, :, 1] / mid_rgb #BLUE\n",
        "        VAL_XX[i, :, :, 3] = data_ndvi #NDVI\n",
        "        VAL_XX[i, :, :, 4] = 1- data[:, :, 13] / mid_slope #SLOPE\n",
        "        VAL_XX[i, :, :, 5] = 1 - data[:, :, 13] / mid_elevation #ELEVATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uyhkZbA0piR"
      },
      "source": [
        "Prediction for validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUi2mWmH0mlv"
      },
      "outputs": [],
      "source": [
        "\n",
        "threshold = 0.5\n",
        "pred_img = model.predict(VAL_XX)\n",
        "pred_img = (pred_img > threshold).astype(np.uint8)\n",
        "pred_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHrLoJdoH3qi"
      },
      "outputs": [],
      "source": [
        "img = 17\n",
        "fig,(ax1,ax2)= plt.subplots(1,2,figsize=(15,10))\n",
        "ax1.imshow(pred_img[img, :, :, 0])\n",
        "ax1.set_title(\"Predictions\")\n",
        "ax2.imshow(VAL_XX[img, :, :, 0:3])\n",
        "ax2.set_title('Training Image')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZG6t4K50vn1"
      },
      "source": [
        "Visualization Of Validation Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-z_oHMv0rz2"
      },
      "outputs": [],
      "source": [
        "Detect = 1\n",
        "def landslide_detector(Detect):\n",
        "     if(Detect):\n",
        "       return('/content/gdrive/MyDrive/f.png','/content/gdrive/MyDrive/f2.png','/content/gdrive/MyDrive/f3.png')\n",
        "     else:\n",
        "        return('/content/gdrive/MyDrive/f4.png','/content/gdrive/MyDrive/f4.png','/content/gdrive/MyDrive/f4.png')\n",
        "\n",
        "demo = gr.Interface(fn=landslide_detector,\n",
        "                    inputs=\"checkbox\",\n",
        "                    outputs=[\"image\",\"image\",\"image\"])\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1qH3Mvc1HbM"
      },
      "outputs": [],
      "source": [
        "write_directory = r'/content/gdrive/MyDrive/ValidData/mask'\n",
        "for i, name in enumerate(mask_name):\n",
        "  h5f = h5py.File(write_directory + \"/\" + name, 'w')\n",
        "  # change the dimention of prediction to (n, 128, 128)\n",
        "  pred_mask = pred_img[i, :, :, 0]\n",
        "\n",
        "  # write to the directory\n",
        "  h5f.create_dataset('mask', data = pred_mask)\n",
        "  h5f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}